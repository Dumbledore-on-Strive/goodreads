{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handy-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-flashing",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "everyday-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.goodreads.com/list/show/1043.Books_That_Should_Be_Made_Into_Movies'\n",
    "\n",
    "def get_books_url_per_page(base_url, tag, tag_class):\n",
    "\n",
    "    book_url_list = []\n",
    "    base_page = rq.get(base_url)\n",
    "    base_content = BeautifulSoup(base_page.content, 'html.parser')\n",
    "\n",
    "    for a in base_content.find_all(tag, class_= tag_class):\n",
    "        book_url_list.append('https://www.goodreads.com/' + a['href'])\n",
    "    \n",
    "    return book_url_list\n",
    "\n",
    "\n",
    "def get_base_page_list(base_url, n):\n",
    "    base_page_list = []\n",
    "    for i in range(n):\n",
    "        base_page_list.append(base_url + str(i + 1))\n",
    "    \n",
    "    return base_page_list\n",
    "\n",
    "whole_book_url_list = []\n",
    "\n",
    "for link in get_base_page_list('https://www.goodreads.com/list/show/1043.Books_That_Should_Be_Made_Into_Movies?page=', 10):\n",
    "    whole_book_url_list.append(get_books_url_per_page(link, 'a', 'bookTitle'))\n",
    "\n",
    "whole_book_url_list = [link for subs in whole_book_url_list for link in subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boxed-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_book_url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-making",
   "metadata": {},
   "source": [
    "# All functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "biological-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(page_soup):\n",
    "    try :\n",
    "        title = page_soup.find('h1', {\"id\":\"bookTitle\"}).text.strip()\n",
    "        return title\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complicated-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(page_soup):\n",
    "    try :\n",
    "        author = page_soup.find(\"a\", {\"class\":\"authorName\"}).text.strip()\n",
    "        return author\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recent-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_rating(page_soup):\n",
    "    try :\n",
    "        num_rating=page_soup.find(itemprop=\"ratingCount\").text.strip().replace(\"ratings\", \"\").replace(\",\", \"\")\n",
    "        return int(num_rating)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abandoned-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_reviews(page_soup):\n",
    "    try :\n",
    "        num_review=page_soup.find(itemprop=\"reviewCount\").text.replace(\"reviews\", \"\").replace(\",\", \"\").strip()\n",
    "        return int(num_review)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "threaded-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_rating(page_soup):\n",
    "    try :\n",
    "        avg_rating=page_soup.find(\"span\", itemprop=\"ratingValue\").text.strip()\n",
    "        return float(avg_rating)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "plain-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pages(page_soup):\n",
    "    try :\n",
    "        num_pages=page_soup.find(itemprop=\"numberOfPages\").text.strip().replace(\"pages\", \"\")\n",
    "        return int(num_pages)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "israeli-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_publish_year(page_soup):\n",
    "    try:\n",
    "        original_publish_year=page_soup.find_all(\"div\", class_=\"row\")[1].text.split()\n",
    "        for i in original_publish_year:\n",
    "            if i.isnumeric()==True:\n",
    "                return  i\n",
    "    except:\n",
    "        return  np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surgical-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series(page_soup):\n",
    "\n",
    "        try:\n",
    "\n",
    "            series = page_soup.find(id=\"bookSeries\").text.strip() \n",
    "            if len(series)!= 0:\n",
    "                return True\n",
    "            else: \n",
    "                return False\n",
    "        except:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polish-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genres(page_soup):\n",
    "    try:\n",
    "        g_list=[]\n",
    "        genres = page_soup.find('div',class_=\"rightContainer\").find_all(class_=\"left\")\n",
    "        for row in genres:\n",
    "            row=row.text.replace(\">\",\"\").strip().split()\n",
    "            row=\" \".join(row)\n",
    "            g_list.append(row)\n",
    "        return g_list\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collective-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award(page_soup):\n",
    "    try:\n",
    "        count=0\n",
    "        awards = page_soup.find(\"div\", {\"itemprop\": \"awards\"}).find_all('a')\n",
    "        for award in awards:\n",
    "            if award!=None:\n",
    "                count+=1\n",
    "        return count\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "trained-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_place(page_soup):\n",
    "    try:\n",
    "        place = page_soup.find(\"div\", {'id':\"bookDataBox\"}).find('span',class_=\"darkGreyText\").text.replace(\"(\",\"\").replace(\")\",\"\").strip()\n",
    "        return str(place)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-literacy",
   "metadata": {},
   "source": [
    "# Create a dictionary and csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "material-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_frame(links):\n",
    "    data_frame=[]\n",
    "    for url  in  links:\n",
    "        page_html = rq.get(url)\n",
    "        page_soup = BeautifulSoup(page_html.content, 'html.parser')\n",
    "        title=get_title(page_soup)\n",
    "      \n",
    "        author=get_author(page_soup)\n",
    "      \n",
    "        num_rating=get_num_rating(page_soup)\n",
    "       \n",
    "        num_reviews=get_num_reviews(page_soup)\n",
    "       \n",
    "        avg_rating=get_avg_rating(page_soup)\n",
    "        \n",
    "        num_pages=get_num_pages(page_soup)\n",
    "        original_publish_year=get_original_publish_year(page_soup)\n",
    "        series=get_series(page_soup)\n",
    "       \n",
    "        genres=get_genres(page_soup)\n",
    "       \n",
    "        award=get_award(page_soup)\n",
    "       \n",
    "        place=get_place(page_soup)\n",
    "        dictionary={\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"num_rating\": num_rating,\n",
    "            \"num_reviews\": num_reviews,\n",
    "            \"avg_rating\": avg_rating,\n",
    "            \"num_pages\": int(num_pages),\n",
    "            \"original_publish_year\": original_publish_year,\n",
    "            \"series\": series,\n",
    "            \"genres\": genres,\n",
    "            \"award\": int(award),\n",
    "            \"place\": place,\n",
    "            \"url\": url\n",
    "        }\n",
    "        data_frame.append(dictionary)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_frame(whole_book_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "  \n",
    "# saving the dataframe\n",
    "df.to_csv('goodreads_final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-participant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
